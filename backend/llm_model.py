import os
import json
import requests
from langchain_upstage import ChatUpstage
from langchain.tools import tool
from datetime import datetime, timezone
from difflib import SequenceMatcher
import re
from datetime import datetime
from dateutil.parser import parse


# âœ… API í‚¤ ì„¤ì •
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
os.environ["UPSTAGE_API_BASE"] = "https://api.upstage.ai/v1/solar"
llm = ChatUpstage(api_key=UPSTAGE_API_KEY, model="solar-pro")

#ì¶”ê°€ë¨ë¨
from langchain.memory import ConversationBufferMemory
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate


template = """
ë„ˆëŠ” ì¥í•™ê¸ˆ ì¶”ì²œ ì „ë¬¸ê°€ì•¼.

ë‹¤ìŒì€ ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ê¸°ë¡ì´ì•¼:
{chat_history}

ì‚¬ìš©ìì˜ ì…ë ¥: {input}

ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì¡°ê±´ë“¤ì„ ê³ ë ¤í•´ì„œ, ì ì ˆí•œ ì¥í•™ê¸ˆ/ì§€ì›ê¸ˆ í”„ë¡œê·¸ë¨ì„ ì¶”ì²œí•˜ê±°ë‚˜
ì¶”ê°€ì ìœ¼ë¡œ í•„ìš”í•œ ì •ë³´ë¥¼ ì •ì¤‘í•˜ê²Œ ì§ˆë¬¸í•´.

í•­ìƒ ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ëŠ” í†¤ìœ¼ë¡œ ëŒ€ë‹µí•´.
"""

temp = PromptTemplate(input_variables=["chat_history", "input"], template=template)




# âœ… ì‹ ë¢° ë„ë©”ì¸
TRUSTED_DOMAINS = ["go.kr", "or.kr", "korea.kr", "work.go.kr", "bokjiro.go.kr", "welfare.seoul.kr", "seoul.go.kr", "kosaf.go.kr", "kofac.re.kr", "kicoa.or.kr"]

# âœ… ìœ íš¨í•œ ë§í¬ í•„í„°ë§
# âœ… NAVER API ê¸°ë°˜ ë§í¬ ê²€ìƒ‰
def compute_title_similarity(a: str, b: str) -> float:
    return SequenceMatcher(None, a, b).ratio()

from datetime import datetime, timezone

def search_naver_url(program_title):
    url = "https://openapi.naver.com/v1/search/webkr.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }

    best_link = ""
    best_score = -1

    now = datetime.now(timezone.utc)  # âœ… ê²½ê³  ì—†ëŠ” UTC ê¸°ì¤€ í˜„ì¬ ì‹œê°„

    for domain in TRUSTED_DOMAINS:
        params = {
            "query": f"{program_title} site:{domain}",
            "display": 5,
            "sort": "date"
        }

        try:
            res = requests.get(url, headers=headers, params=params, timeout=5)
            data = res.json()

            for item in data.get("items", []):
                link = item.get("link", "").strip("<>")
                naver_title = item.get("title", "").replace("<b>", "").replace("</b>", "")
                pub_date_str = item.get("pubDate", "")

                if not link.startswith("http"):
                    continue

                # âœ… ì†ë„ í–¥ìƒ: ë§í¬ ìœ íš¨ì„± ê²€ì¦ ìƒëµ (ì›í•˜ë©´ fallbackì—ì„œ ê²€ì¦ ê°€ëŠ¥)
                try:
                    pub_date = datetime.strptime(pub_date_str, "%a, %d %b %Y %H:%M:%S %z")
                    recency_score = max(0, 1 - (now - pub_date).days / 30)
                except:
                    recency_score = 0

                title_similarity = compute_title_similarity(program_title, naver_title)

                total_score = 0.2 * title_similarity + 0.8 * recency_score

                if total_score > best_score:
                    best_score = total_score
                    best_link = link

        except Exception as e:
            print(f"âŒ NAVER ê²€ìƒ‰ ì˜¤ë¥˜ (ë„ë©”ì¸: {domain}): {e}")

    return best_link or f"https://www.google.com/search?q={program_title}+ì¥í•™ê¸ˆ"



# âœ… ë‹¨ê³„ 1: LLMì´ ì¥í•™ê¸ˆ ì œëª©/ì„¤ëª… ì¶”ì²œ
def generate_program_titles(user_input: str):
    prompt = f"""
    ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì •ë³´ì…ë‹ˆë‹¤: "{user_input}"

    ì‚¬ìš©ìì˜ ì¡°ê±´ì— ë”°ë¼ ì ì ˆí•œ ì¥í•™ê¸ˆ ë˜ëŠ” ì§€ì› í”„ë¡œê·¸ë¨ì„ ìµœì†Œ 20ê°œ ì¶”ì²œí•´ì£¼ì„¸ìš”.
    ê° í•­ëª©ì€ ì œëª©(title)ê³¼ ì„¤ëª…(description)ë§Œ í¬í•¨í•˜ê³ , ë§í¬ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

    ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”:
    {{
      "programs": [
        {{"title": "...", "description": "..."}},
        ...
      ]
    }}
    """
    result = llm.invoke(prompt)
    try:
        raw_content = result.content

        data = json.loads(raw_content)
        return data.get("programs", [])
    except Exception as e:
        return e

# âœ… ë‹¨ê³„ 2: NAVER APIë¥¼ í†µí•œ ë§í¬ ë§¤í•‘
def enrich_with_links(programs):
    enriched = []
    for p in programs:
        title = p["title"]
        link = search_naver_url(title)
        enriched.append({
            "title": title,
            "description": p.get("description", ""),
            "link": link or "âš ï¸ ìœ íš¨í•œ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        })
    return enriched

# âœ… ë‹¨ê³„ 3: DBê¸°ë°˜ RAGì´ìš©

from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings  # or Upstage, HuggingFace ë“±
from langchain.schema import Document
import pandas as pd

csv_path = "D:\\code\\2025-1\\AGI_Hackathon2\\AGI-Hackaton\\backend\\scholarships.csv"
df = pd.read_csv(csv_path)

df_clean = df.dropna(subset=["name", "raw_text"])

# Document ë¦¬ìŠ¤íŠ¸ ìƒì„±
docs = [
    Document(
        page_content=f"{row['name']} - {row['raw_text']}",
        metadata={
            "id": row["id"],
            "name": row["name"]
        }
    )
    for _, row in df_clean.iterrows()
]

embedding = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli")
vectorstore = FAISS.from_documents(docs, embedding)
retriever = vectorstore.as_retriever()

def recommend_by_rag(user_input: str):
    retrieved_docs = retriever.get_relevant_documents(user_input)

    context = "\n\n".join([doc.page_content for doc in retrieved_docs])

    prompt = f"""
    ë‹¤ìŒì€ ì¥í•™ê¸ˆ ì •ë³´ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ëœ ë‚´ìš©ì…ë‹ˆë‹¤:
    {context}

    ì‚¬ìš©ìì˜ ì¡°ê±´: "{user_input}"

    ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì í•©í•œ ì¥í•™ê¸ˆ 3ê°€ì§€ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.
    ê° í•­ëª©ì€ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ê³ , ì œëª©(title), ì„¤ëª…(description), ë§í¬(url)ì„ í¬í•¨í•˜ì„¸ìš”.

    ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”:
    {{
      "programs": [
        {{"title": "...", "description": "...", "url": "..."}},
        ...
      ]
    }}
    """

    result = llm.invoke(prompt)
    return result.content  # JSON ë¬¸ìì—´





# âœ… Streamlit UI


def initial_input(base_input: str):
    rag_result = recommend_by_rag(base_input)
    rag_result = json.loads(rag_result)
    output = rag_result["programs"]
    return output
    
    
# base_input = 'ì‚¬ìš©ìì •ë³´'
# st.session_state.chat_history.append(("ai", json.loads(recommend_by_rag(base_input))["programs"]))
# for role, msg in st.session_state.chat_history:
#     with st.chat_message("ğŸ§‘" if role == "user" else "ğŸ¤–"):
#         if role == "ai":
#             for p in msg:
#                 link = p.get('link', '')
#                 st.markdown(f"### ğŸ“Œ {p['title']}")
#                 st.markdown(p['description'])
#                 if isinstance(link, str) and link.startswith("http"):
#                     st.markdown(f"ğŸ”— [ê³µì‹ ë§í¬]({link})")
#                 else:
#                     st.markdown("âš ï¸ ìœ íš¨í•œ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
#         else:
#             st.markdown(msg)


# # âœ… ì‚¬ìš©ì ì…ë ¥
# user_input = st.chat_input("ì˜ˆ: ì´ê³µê³„ì—´ ëŒ€í•™ìƒ 3í•™ë…„ì…ë‹ˆë‹¤. ì–´ë–¤ ì¥í•™ê¸ˆì´ ìˆë‚˜ìš”?")

# if user_input:
#     st.session_state.chat_history.append(("user", user_input))

#     with st.spinner("Step 1ï¸âƒ£: ì¡°ê±´ì— ë§ëŠ” í”„ë¡œê·¸ë¨ì„ ì°¾ëŠ” ì¤‘..."):
#         programs = generate_program_titles(user_input)

#     with st.spinner("Step 2ï¸âƒ£: ê³µì‹ ë§í¬ë¥¼ ê²€ìƒ‰ ì¤‘..."):
#         enriched_programs = enrich_with_links(programs)

#     with st.spinner("Step 3ï¸âƒ£: ì‹¤ì œ ìœ íš¨í•œ ë§í¬ì™€ ì¼ì¹˜í•˜ëŠ” í”„ë¡œê·¸ë¨ í•„í„°ë§ ì¤‘..."):
#         DBdata = recommend_by_rag(user_input)
    

#     DBdata = json.loads(DBdata)

#     merged_programs = DBdata["programs"] + enriched_programs

#     st.session_state.chat_history.append(("ai", merged_programs))

# # âœ… ëŒ€í™” ì¶œë ¥
# for role, msg in st.session_state.chat_history:
#     with st.chat_message("ğŸ§‘" if role == "user" else "ğŸ¤–"):
#         if role == "ai":
#             for p in msg:
#                 link = p.get('link', '')
#                 st.markdown(f"### ğŸ“Œ {p['title']}")
#                 st.markdown(p['description'])
#                 if isinstance(link, str) and link.startswith("http"):
#                     st.markdown(f"ğŸ”— [ê³µì‹ ë§í¬]({link})")
#                 else:
#                     st.markdown("âš ï¸ ìœ íš¨í•œ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
#         else:
#             st.markdown(msg)